{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMy10e5hUxCrkHgkmvhas9S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namankathuria21/REGRESSION/blob/main/REGRESSION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression Assignment\n",
        "\n",
        "Q1. What is Simple Linear Regression?\n",
        "\n",
        "Simple Linear Regression is a statistical method used to model the relationship between one independent variable (X) and one dependent variable (Y).\n",
        "\n",
        "Equation: Y = mX + c\n",
        "\n",
        "Purpose: To predict Y using X and determine how X influences Y.\n",
        "\n",
        "Q2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "Linearity: Relationship between X and Y is linear.\n",
        "\n",
        "Independence: Observations are independent.\n",
        "\n",
        "Homoscedasticity: Constant variance of residuals.\n",
        "\n",
        "Normality: Residuals follow a normal distribution.\n",
        "\n",
        "No strong outliers.\n",
        "\n",
        "Q3. What does the coefficient m represent in the equation Y = mX + c?\n",
        "\n",
        "The slope (m) represents the rate of change in Y for a one-unit change in X.\n",
        "\n",
        "Example: If m = 2, then each increase of 1 unit in X increases Y by 2.\n",
        "\n",
        "Q4. What does the intercept c represent in the equation Y = mX + c?\n",
        "\n",
        "The intercept (c) is the expected value of Y when X = 0. It provides the baseline starting point of the regression line.\n",
        "\n",
        "Q5. How do we calculate the slope m in Simple Linear Regression?\n",
        "𝑚\n",
        "=\n",
        "∑\n",
        "(\n",
        "𝑋\n",
        "𝑖\n",
        "−\n",
        "𝑋\n",
        "ˉ\n",
        ")\n",
        "(\n",
        "𝑌\n",
        "𝑖\n",
        "−\n",
        "𝑌\n",
        "ˉ\n",
        ")\n",
        "∑\n",
        "(\n",
        "𝑋\n",
        "𝑖\n",
        "−\n",
        "𝑋\n",
        "ˉ\n",
        ")\n",
        "2\n",
        "m=\n",
        "∑(X\n",
        "i\n",
        "\t​\n",
        "\n",
        "−\n",
        "X\n",
        "ˉ\n",
        ")\n",
        "2\n",
        "∑(X\n",
        "i\n",
        "\t​\n",
        "\n",
        "−\n",
        "X\n",
        "ˉ\n",
        ")(Y\n",
        "i\n",
        "\t​\n",
        "\n",
        "−\n",
        "Y\n",
        "ˉ\n",
        ")\n",
        "\t​\n",
        "\n",
        "\n",
        "This formula minimizes the squared errors between predicted and actual Y values.\n",
        "\n",
        "Q6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "Least Squares minimizes the sum of squared differences between observed and predicted Y values, ensuring the best-fitting regression line.\n",
        "\n",
        "Q7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "\n",
        "R² measures how much variance in Y is explained by X.\n",
        "\n",
        "Range: 0 to 1.\n",
        "\n",
        "Example: R² = 0.80 → 80% of variation in Y is explained by X.\n",
        "\n",
        "Q8. What is Multiple Linear Regression?\n",
        "\n",
        "Multiple Linear Regression models the relationship between one dependent variable and multiple independent variables.\n",
        "\n",
        "Equation: Y = b₀ + b₁X₁ + b₂X₂ + ... + bₙXₙ + ε\n",
        "\n",
        "Q9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "Simple Linear Regression → One predictor (X).\n",
        "\n",
        "Multiple Linear Regression → Two or more predictors (X₁, X₂, …).\n",
        "\n",
        "Q10. What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "Linearity between predictors and outcome.\n",
        "\n",
        "Independence of errors.\n",
        "\n",
        "Homoscedasticity.\n",
        "\n",
        "Normal distribution of residuals.\n",
        "\n",
        "No multicollinearity (independent variables shouldn’t be highly correlated).\n",
        "\n",
        "Q11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "Heteroscedasticity: Residuals have unequal variance.\n",
        "\n",
        "Effect: Leads to inefficient estimates, biased standard errors, and invalid hypothesis tests.\n",
        "\n",
        "Q12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "Remove highly correlated variables.\n",
        "\n",
        "Use Principal Component Analysis (PCA).\n",
        "\n",
        "Use Ridge or Lasso Regression (regularization).\n",
        "\n",
        "Q13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "One-Hot Encoding\n",
        "\n",
        "Label Encoding\n",
        "\n",
        "Dummy Variables\n",
        "\n",
        "Target Encoding\n",
        "\n",
        "Q14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "Interaction terms capture the combined effect of two variables.\n",
        "Example: Income × Education may explain salary better than individual effects.\n",
        "\n",
        "Q15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "Simple: Intercept = Expected Y when X = 0.\n",
        "\n",
        "Multiple: Intercept = Expected Y when all predictors = 0 (may not always be meaningful).\n",
        "\n",
        "Q16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "Slope shows how much Y changes with a one-unit change in X (holding other variables constant in multiple regression).\n",
        "\n",
        "Q17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "It provides a baseline value of Y when predictors are absent (X = 0).\n",
        "\n",
        "Q18. What are the limitations of using R² as a sole measure of model performance?\n",
        "\n",
        "High R² doesn’t guarantee causation.\n",
        "\n",
        "Adding more predictors always increases R² (can be misleading).\n",
        "\n",
        "Doesn’t measure overfitting.\n",
        "\n",
        "Q19. How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "Large SE → Coefficient is unstable and unreliable.\n",
        "\n",
        "Suggests multicollinearity or insufficient data.\n",
        "\n",
        "Q20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "Identified when residuals form a funnel shape in scatter plots.\n",
        "\n",
        "Important to fix → prevents invalid p-values and confidence intervals.\n",
        "\n",
        "Q21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "\n",
        "High R²: Many variables explain variance.\n",
        "\n",
        "Low Adjusted R²: Extra predictors are not actually useful (overfitting).\n",
        "\n",
        "Q22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "Ensures comparability of coefficients.\n",
        "\n",
        "Improves numerical stability.\n",
        "\n",
        "Essential for models with regularization (Ridge/Lasso).\n",
        "\n",
        "Q23. What is polynomial regression?\n",
        "\n",
        "Polynomial Regression models non-linear relationships by including polynomial terms of predictors.\n",
        "\n",
        "Q24. How does polynomial regression differ from linear regression?\n",
        "\n",
        "Linear regression: Straight-line fit.\n",
        "\n",
        "Polynomial regression: Curved line fit using squared, cubic, etc. terms.\n",
        "\n",
        "Q25. When is polynomial regression used?\n",
        "\n",
        "When the relationship between variables is non-linear but can be approximated by a polynomial curve.\n",
        "\n",
        "Q26. What is the general equation for polynomial regression?\n",
        "𝑌\n",
        "=\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝑏\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝑏\n",
        "3\n",
        "𝑋\n",
        "3\n",
        "+\n",
        ".\n",
        ".\n",
        ".\n",
        "+\n",
        "𝑏\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "+\n",
        "𝜀\n",
        "Y=b\n",
        "0\n",
        "\t​\n",
        "\n",
        "+b\n",
        "1\n",
        "\t​\n",
        "\n",
        "X+b\n",
        "2\n",
        "\t​\n",
        "\n",
        "X\n",
        "2\n",
        "+b\n",
        "3\n",
        "\t​\n",
        "\n",
        "X\n",
        "3\n",
        "+...+b\n",
        "n\n",
        "\t​\n",
        "\n",
        "X\n",
        "n\n",
        "+ε\n",
        "Q27. Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "Yes. Multivariate polynomial regression includes polynomial terms of multiple predictors.\n",
        "\n",
        "Q28. What are the limitations of polynomial regression?\n",
        "\n",
        "High degree → overfitting.\n",
        "\n",
        "Sensitive to outliers.\n",
        "\n",
        "Hard to interpret.\n",
        "\n",
        "Q29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "Adjusted R²\n",
        "\n",
        "Cross-validation\n",
        "\n",
        "AIC/BIC (Information Criteria)\n",
        "\n",
        "Residual analysis\n",
        "\n",
        "Q30. Why is visualization important in polynomial regression?\n",
        "\n",
        "Helps confirm non-linearity.\n",
        "\n",
        "Shows whether polynomial degree is appropriate.\n",
        "\n"
      ],
      "metadata": {
        "id": "Gm80lV294KZx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVgAUNTq4GEp"
      },
      "outputs": [],
      "source": [
        "#Q31. How is polynomial regression implemented in Python?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Sample data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1,1)\n",
        "y = np.array([2, 5, 10, 17, 26])\n",
        "\n",
        "# Polynomial transformation\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit regression\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "plt.scatter(X, y, color=\"blue\")\n",
        "plt.plot(X, y_pred, color=\"red\")\n",
        "plt.title(\"Polynomial Regression\")\n",
        "plt.show()"
      ]
    }
  ]
}